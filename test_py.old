
import torch
from transformers import AutoModel,AutoTokenizer,AutoModelForCausalLM
import os
import flask
from flask import Flask,request,render_template
import webbrowser

model_path = "../DeepSeek-Coder"
print("CUDA Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0))
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
    low_cpu_mem_usage=True,
    offload_folder="offload"
)

print("Free Mem:", torch.cuda.mem_get_info()[0] // (1024**2), "MB")
#messages=[{"role": "user", "content": ""}]

def trim_output1(lines):
    start = next(i for i, line in enumerate(lines) if "```" in line or "python" in line)
    end = next(i for i, line in enumerate(lines[start + 1:], start + 1) if "```" in line) 

def GenOutput(message):
    prompt = tokenizer.apply_chat_template(
        message,
        add_generation_prompt=True,
        return_tensors="pt"
        )
    attention_mask = (prompt != tokenizer.pad_token_id).long()
    
    prompt = prompt.to(model.device)
    attention_mask = attention_mask.to(model.device)
    
    outputs = model.generate(
            input_ids=prompt,
            attention_mask=attention_mask,
            max_new_tokens=1024,
            temperature=0.7,
            top_p=0.95,
            #top_k=50,
            #repetition_penalty=1.1,
            do_sample=True,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
    response = tokenizer.decode(outputs[0][prompt.shape[1]:], skip_special_tokens=True)
    return response

text = input()
messages = [{"role": "user", "content": text}]
response1= GenOutput(messages)
print("Output 1 Generated")
"""
messages = [
    {"role": "user", "content": f"Convert the into a valid Graphviz python code (use HTML <TABLE> format if required, Python code) that I can run with graphviz library:\n\n{response1}"}
]
response2 = GenOutput(messages)"""
#print("Output 2 Generated")
file_path = "empty.py"
trim_output1(response1,target_file=file_path)
print(response1)

"""
app = Flask(__name__)


@app.route("/", methods=["GET", "POST"])
def TakeInput():
   
    if request.method == "POST":
        text = request.form.get('result')
        if not text:
            return render_template("frontend.html", prompt="", response="No input provided.")
    

        messages = [{"role": "user", "content": text}]
        response1= GenOutput(messages)
        print("Output 1 Generated")

        messages = [
            {"role": "user", "content": f"Convert the into a valid Graphviz table (use HTML <TABLE> format if required, Python code) that I can run with graphviz library:\n\n{response1}"}
        ]
        response2 = GenOutput(messages)
        print("Output 2 Generated")
    
        file_path = "empty.py"
        reset_and_extract_from_string(response2,target_file=file_path)
    
        return render_template("frontend.html",prompt = text, response=response2)
    else:
        return render_template("frontend.html", prompt="", response="")


if __name__ == "__main__":
    webbrowser.open("http://127.0.0.1:5000")
    app.run(debug=True,port = 3000)
"""

"""
outputs=[]
prompt=""
response=""
def GenOutput(message):
    prompt = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
    )
    attention_mask = (prompt != tokenizer.pad_token_id).long()

    prompt = prompt.to(model.device)
    attention_mask = attention_mask.to(model.device)

    outputs = model.generate(
        input_ids=prompt,
        attention_mask=attention_mask,
        max_new_tokens=1024,
        temperature=0.7,
        top_p=0.95,
        #top_k=50,
        #repetition_penalty=1.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id,
    )
    response = tokenizer.decode(outputs[0][prompt.shape[1]:], skip_special_tokens=True)
    return response

response1= GenOutput(messages)


print("Output 1 Generated")

messages = [
    {"role": "user", "content": f"Convert the into a valid Graphviz table (use HTML <TABLE> format if required, Python code) that I can run with graphviz library:\n\n{response1}"}
]

response2 = GenOutput(messages)

print("Output 2 Generated")
file_path = "empty.py"

def reset_and_extract_from_string(data, target_file="empty.py"):
    # Step 1: Delete the file if it exists
    if os.path.exists(target_file):
        os.remove(target_file)

    # Step 2: Create a new empty file
    open(target_file, "w").close()

    # Step 3: Split string into lines
    lines = data.splitlines(keepends=True)

    # Step 4: Find first ```
    try:
        start = next(i for i, line in enumerate(lines) if "```" in line)
        end = next(i for i, line in enumerate(lines[start + 1:], start + 1) if "```" in line)
        extracted = lines[start + 1:end]
    except StopIteration:
        extracted = []

    # Step 5: Write extracted content to file
    with open(target_file, "w") as tgt:
        tgt.writelines(extracted)

reset_and_extract_from_string(response2)

print(response2)
"""
